{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RogShotz/CSC-482-Project/blob/model-testing/CSC482_Project_Geneology_Extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMQQKY_UAQH6"
      },
      "source": [
        "**Geneology Extractor**\n",
        "\n",
        "---\n",
        "\n",
        "Here we go I guess. Here's some wiki stuff for extracting articles."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Packages and Imports**"
      ],
      "metadata": {
        "id": "SSqRnBPKDpf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install spacy-transformers\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "7gShbavAu1Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgbtKSGGDU11",
        "outputId": "dd81dab0-2c78-447f-fc52-1d60b98b669e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk, csv, spacy\n",
        "from google.colab import drive\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.classify import accuracy\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DataSets and Setup**\n",
        "\n",
        "Datasets are located at the google drive here\n",
        "\n",
        "https://drive.google.com/drive/folders/1NVlU3sMEsQrF7SBM-F6SIbx27N91FKfj?usp=sharing"
      ],
      "metadata": {
        "id": "X7CQpqzbUzZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "F9Vbnk2qNFGw"
      },
      "outputs": [],
      "source": [
        "# Function Definitions\n",
        "\n",
        "# Download the spaCy model for NER\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "# Dataset extractions, mount Google Drive before running\n",
        "def extract_columns_from_csv(file_path, has_labels=True):\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "        csv_reader = csv.reader(csvfile, quoting=csv.QUOTE_MINIMAL)\n",
        "        if has_labels:\n",
        "          next(csv_reader)  # Skip the header row\n",
        "\n",
        "        for row in csv_reader:\n",
        "            if has_labels:\n",
        "                text, label = row\n",
        "                labels.append(label)\n",
        "            else:\n",
        "                text = row[0]\n",
        "\n",
        "            texts.append(text)\n",
        "\n",
        "    if has_labels:\n",
        "        return texts, labels\n",
        "    else:\n",
        "        return texts\n",
        "\n",
        "\n",
        "# Tokenize and preprocess text data\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    words = [word for word in words if word.isalnum()]  # Remove non-alphanumeric characters\n",
        "    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
        "    return dict([(word, True) for word in words])\n",
        "\n",
        "# Extracts named entities from the text\n",
        "def extract_people(sentences):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    people_list = []\n",
        "    for sentence in sentences:\n",
        "        doc = nlp(sentence)\n",
        "        people_in_sentence = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
        "        if people_in_sentence:\n",
        "            people_list.append(people_in_sentence)\n",
        "        else:\n",
        "            people_list.append(None)\n",
        "\n",
        "    return people_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**\n",
        "\n",
        "---\n",
        "\n",
        "Change the file path to use the data file that you want to train with, this must be labelled data"
      ],
      "metadata": {
        "id": "d7CDv4e00u3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/CSC482_data_sets/son.csv'\n",
        "texts = extract_columns_from_csv(file_path)"
      ],
      "metadata": {
        "id": "LPqO0nA7QG3I"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the labeled data in the required format for NLTK\n",
        "featuresets = [(preprocess_text(text), label) for text, label in zip(texts[0], texts[1])]\n",
        "label_set = set(texts[1])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_set, test_set = train_test_split(featuresets, test_size=0.2, random_state=23)\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "nb_classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Get the accuracy of the model\n",
        "accuracy_result = accuracy(nb_classifier, test_set)\n",
        "print(f\"Accuracy: {accuracy_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXDrdsKOy30z",
        "outputId": "761ebe8c-660a-4a85-a527-7a359589da6d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More informative printouts for testing**"
      ],
      "metadata": {
        "id": "57_iuiC8ETJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the test set and probabilities\n",
        "print(\"Text has stop words filtered out\\n------\")\n",
        "label_set = set(texts[1])\n",
        "neg_label = next((s for s in label_set if s.startswith('not')), None)\n",
        "pos_label = next((s for s in label_set if not s.startswith('not')), None)\n",
        "for i, (text_features, _) in enumerate(test_set):\n",
        "    if i >= 10:\n",
        "        break\n",
        "    prob_dist = nb_classifier.prob_classify(text_features)\n",
        "    print(f\"Text: {' '.join(text_features.keys())}\")\n",
        "    print(f\"Probability of having 'son' relation: {prob_dist.prob(pos_label)}\")\n",
        "    print(f\"Probability of not having 'son' relation: {prob_dist.prob(neg_label)}\")\n",
        "    print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0BiyoOHEW88",
        "outputId": "a3c3e031-849e-430f-bac6-2111d4bc4592"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text has stop words filtered out\n",
            "------\n",
            "Text: historian meticulously cataloged artifacts ensuring preservation future generations\n",
            "Probability of having 'son' relation: 0.0071874641884249035\n",
            "Probability of not having 'son' relation: 0.9928125358115756\n",
            "------\n",
            "Text: grace compassionate veterinarian cared animals unwavering dedication making positive impact countless furry companions\n",
            "Probability of having 'son' relation: 0.0009946160199898856\n",
            "Probability of not having 'son' relation: 0.9990053839800108\n",
            "------\n",
            "Text: sophie christopher adopted rescue dog family\n",
            "Probability of having 'son' relation: 0.000947254678231364\n",
            "Probability of not having 'son' relation: 0.999052745321767\n",
            "------\n",
            "Text: boy father built model airplane together\n",
            "Probability of having 'son' relation: 0.9979349723607761\n",
            "Probability of not having 'son' relation: 0.002065027639222439\n",
            "------\n",
            "Text: quiet village james son benjamin tended family farm cultivating deep connection land traditions\n",
            "Probability of having 'son' relation: 0.8518279119993933\n",
            "Probability of not having 'son' relation: 0.1481720880006076\n",
            "------\n",
            "Text: natalie jacob went date art museum\n",
            "Probability of having 'son' relation: 0.9996841619453756\n",
            "Probability of not having 'son' relation: 0.0003158380546242762\n",
            "------\n",
            "Text: sarah james went family vacation beach\n",
            "Probability of having 'son' relation: 0.00016813220510724473\n",
            "Probability of not having 'son' relation: 0.9998318677948942\n",
            "------\n",
            "Text: charlotte supported son henry soccer game\n",
            "Probability of having 'son' relation: 0.9999823049607466\n",
            "Probability of not having 'son' relation: 1.769503925195865e-05\n",
            "------\n",
            "Text: passionate marine biologist lily explored depths ocean uncovering mysteries underwater ecosystems\n",
            "Probability of having 'son' relation: 0.0003385269477487115\n",
            "Probability of not having 'son' relation: 0.9996614730522518\n",
            "------\n",
            "Text: david emily picnic park\n",
            "Probability of having 'son' relation: 0.015228529875893355\n",
            "Probability of not having 'son' relation: 0.9847714701241073\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Labelling and Getting Relations**"
      ],
      "metadata": {
        "id": "_WRb5Y_TD6gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Inputs:\n",
        "classifier: a trained classifier\n",
        "texts: the data to be classified, list of sentences\n",
        "label_set: set of the labels (\"positive\", \"negative\")\n",
        "people: list of Named Entities in the sentences\n",
        "\n",
        "Outputs:\n",
        "tuple:  -positive label\n",
        "        -probability\n",
        "        -sentence\n",
        "        -named people\n",
        "\"\"\"\n",
        "def get_son_relation(classifier, texts, label_set, people):\n",
        "  neg_label = next((s for s in label_set if s.startswith('not')), None)\n",
        "  pos_label = next((s for s in label_set if not s.startswith('not')), None)\n",
        "  featureset = [preprocess_text(text) for text in texts]\n",
        "\n",
        "  has_relation = []\n",
        "\n",
        "  for i, text_features in enumerate(featureset):\n",
        "    prob_dist = nb_classifier.prob_classify(text_features)\n",
        "    prob_relation = prob_dist.prob(pos_label)\n",
        "\n",
        "    if prob_relation >= 0.7:\n",
        "      has_relation.append((pos_label, prob_relation, texts[i], people[i]))\n",
        "\n",
        "  return has_relation"
      ],
      "metadata": {
        "id": "XV4uuTLK39gd"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change the file_path to your csv file (1 column of strings)**"
      ],
      "metadata": {
        "id": "F6QxWCPuEA1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/CSC482_data_sets/data.csv'\n",
        "texts = extract_columns_from_csv(file_path, has_labels=False)\n",
        "people = extract_people(texts)\n",
        "relations = get_son_relation(nb_classifier, texts, label_set, people)"
      ],
      "metadata": {
        "id": "6uFko-vk_NHq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Information is stored as a list of tuples**"
      ],
      "metadata": {
        "id": "nJ9zCk4FEJVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relations[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHuY0HcOA-6Y",
        "outputId": "f75f24f7-d8c3-498f-f606-13cedfcc25e4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('son',\n",
              " 0.9324277119804167,\n",
              " 'James hugged his son, Matthew, tightly after returning from his long deployment overseas.',\n",
              " ['James', 'Matthew'])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1OlBDLdVr-R84DebnVN0PEk4AR3aP9WFL",
      "authorship_tag": "ABX9TyMgxBBAsEBSH48kummiqB0O",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}