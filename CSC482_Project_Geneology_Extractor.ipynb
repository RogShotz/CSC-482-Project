{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RogShotz/CSC-482-Project/blob/model-testing/CSC482_Project_Geneology_Extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMQQKY_UAQH6"
      },
      "source": [
        "**Geneology Extractor**\n",
        "\n",
        "---\n",
        "\n",
        "Here we go I guess. Here's some wiki stuff for extracting articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgbtKSGGDU11",
        "outputId": "8979e05f-0fa5-4ef6-e1b6-a291bb70a98c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk, csv\n",
        "from google.colab import drive\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.classify import accuracy\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets: https://drive.google.com/drive/folders/1NVlU3sMEsQrF7SBM-F6SIbx27N91FKfj?usp=sharing"
      ],
      "metadata": {
        "id": "X7CQpqzbUzZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "F9Vbnk2qNFGw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "5a4af6f0-cebe-41c4-9b1e-db45dd396ccb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-3ff7199b670d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Function Definitions\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Function Definitions\n",
        "\n",
        "# Dataset extractions, mount Google Drive before running\n",
        "def extract_columns_from_csv(file_path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "        csv_reader = csv.reader(csvfile, quoting=csv.QUOTE_MINIMAL)\n",
        "        next(csv_reader)  # Skip the header row\n",
        "\n",
        "        for row in csv_reader:\n",
        "            text, label = row\n",
        "            texts.append(text)\n",
        "            labels.append(label)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "# Tokenize and preprocess text data\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    words = [word for word in words if word.isalnum()]  # Remove non-alphanumeric characters\n",
        "    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
        "    return dict([(word, True) for word in words])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/CSC482_data_sets/son.csv'\n",
        "texts = extract_columns_from_csv(file_path)\n",
        "\n",
        "\n",
        "# Prepare the labeled data in the required format for NLTK\n",
        "featuresets = [(preprocess_text(text), label) for text, label in zip(texts[0], texts[1])]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_set, test_set = train_test_split(featuresets, test_size=0.2, random_state=23)\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "nb_classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Get the accuracy of the model\n",
        "accuracy_result = accuracy(nb_classifier, test_set)\n",
        "print(f\"Accuracy: {accuracy_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPqO0nA7QG3I",
        "outputId": "095a89b4-b4be-463e-b23f-ea6ec700aa47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print out the test set and probabilities\n",
        "print(\"Text has stop words filtered out\\n------\")\n",
        "label_set = set(texts[1])\n",
        "neg_label = next((s for s in label_set if s.startswith('not')), None)\n",
        "pos_label = next((s for s in label_set if not s.startswith('not')), None)\n",
        "for i, (text_features, _) in enumerate(test_set):\n",
        "    if i >= 10:\n",
        "        break\n",
        "    prob_dist = nb_classifier.prob_classify(text_features)\n",
        "    print(f\"Text: {' '.join(text_features.keys())}\")\n",
        "    print(f\"Probability of having 'son' relation: {prob_dist.prob(pos_label)}\")\n",
        "    print(f\"Probability of not having 'son' relation: {prob_dist.prob(neg_label)}\")\n",
        "    print(\"------\")"
      ],
      "metadata": {
        "id": "av6DinxlPEke"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OlBDLdVr-R84DebnVN0PEk4AR3aP9WFL",
      "authorship_tag": "ABX9TyNa0qX3aodMIJHqMapNwiSA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}